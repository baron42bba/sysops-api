#!/usr/bin/python2.6

# (c) [2013] LinkedIn Corp. All rights reserved.
# Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License.
# You may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

import redis
import os
import sys
import json
import subprocess
import signal
import hashlib
import platform
from optparse import OptionParser
import bz2
import uuid
import time
import platform

#################################################################################################################
class timeout_exception(Exception):
  pass
#################################################################################################################
def timeout_handler(signum, frame):
    raise timeout_exception()
#################################################################################################################
def prune_array_data(prune):
  # Only keep results from the past 50 transfers
  if len(prune["measured"]) >= 50:
    all_collections = sorted(prune["measured"].keys())  
    del prune["measured"][all_collections[0]]
#################################################################################################################
def generate_per_redis_server_execution_cost(redis_server):
  unique_files[redis_server] = {}
  for catagory in ["mean", "median", "std", "var", "min", "max" ]:
    redis_time_data[redis_server]["individual_redis_server_totals"][catagory] = {}

  for execution in redis_time_data[redis_server]["measured"].iterkeys():
    for store_key in redis_time_data[redis_server]["measured"][execution].iterkeys():
      if not unique_files[redis_server].get(store_key):
        unique_files[redis_server][store_key] = []
      unique_files[redis_server][store_key].append(float(redis_time_data[redis_server]["measured"][execution][store_key]))
      if not unique_files['total'].get(store_key):
        unique_files['total'][store_key] = []
      unique_files['total'][store_key].append(float(redis_time_data[redis_server]["measured"][execution][store_key]))

  if platform.system() == "Linux":
    import numpy as np
    for store_key in unique_files[redis_server].iterkeys():
      redis_time_data[redis_server]['individual_redis_server_totals']['mean'][store_key] = '%.3f' % np.mean(unique_files[redis_server][store_key])
      redis_time_data[redis_server]['individual_redis_server_totals']['median'][store_key] = '%.3f' % np.median(unique_files[redis_server][store_key])
      redis_time_data[redis_server]['individual_redis_server_totals']['std'][store_key] = '%.3f' % np.std(unique_files[redis_server][store_key])
      redis_time_data[redis_server]['individual_redis_server_totals']['var'][store_key] = '%.3f' % np.var(unique_files[redis_server][store_key])
      redis_time_data[redis_server]['individual_redis_server_totals']['min'][store_key] = '%.3f' % np.min(unique_files[redis_server][store_key])
      redis_time_data[redis_server]['individual_redis_server_totals']['max'][store_key] = '%.3f' % np.max(unique_files[redis_server][store_key])
#################################################################################################################
def generate_global_redis_server_execution_cost():

  ############################ Calculate per-file global statistics ############################
  if platform.system() == "Linux":
    import numpy as np
    global_per_file_sysops_api_interaction_times = json_data_directory + "global_per_file_sysops_api_interaction_times.json"
    try:
      input_text = open(global_per_file_sysops_api_interaction_times).read()
      redis_time_data['per_file'] = json.loads(input_text.decode('utf-8', 'ignore'))
    except Exception, e:
      print "Sorry, couldnt read from the filesystem.  Creating a new object." + str(e)
      redis_time_data['per_file'] = {}
      redis_time_data['per_file']['measured'] = {}

    prune_array_data(redis_time_data['per_file'])
    redis_time_data['per_file']['measured'][global_start_time] = {}

    for catagory in ["mean", "median", "std", "var", "min", "max", ]:
      redis_time_data['per_file']['measured'][global_start_time][catagory] = {}
  
    for store_key in unique_files['total'].iterkeys():
      redis_time_data['per_file']['measured'][global_start_time]['mean'][store_key] = '%.3f' % np.mean(unique_files['total'][store_key])
      redis_time_data['per_file']['measured'][global_start_time]['median'][store_key] = '%.3f' % np.median(unique_files['total'][store_key])
      redis_time_data['per_file']['measured'][global_start_time]['std'][store_key] = '%.3f' % np.std(unique_files['total'][store_key])
      redis_time_data['per_file']['measured'][global_start_time]['var'][store_key] = '%.3f' % np.var(unique_files['total'][store_key])
      redis_time_data['per_file']['measured'][global_start_time]['min'][store_key] = '%.3f' % np.min(unique_files['total'][store_key])
      redis_time_data['per_file']['measured'][global_start_time]['max'][store_key] = '%.3f' % np.max(unique_files['total'][store_key])
    ############################ Calculate per-file global statistics ############################
    global_per_file_sysops_api_interaction_times = json_data_directory + "global_per_file_sysops_api_interaction_times.json"
    try:
      with open(global_per_file_sysops_api_interaction_times, mode = "w") as fh:
        json.dump(redis_time_data['per_file'], fh, sort_keys=True, indent=3)
    except Exception, e:
      print "We tried to dump to JSON, but couldnt. Sorry." + str(e)



  ############################ Calculate global statistics ############################
  if platform.system() == "Linux":
    import numpy as np
    for catagory in ["mean", "median", "std", "var", "min", "max" ]:
      redis_time_data['global_total']['total_redis_interaction_time'][catagory] = {}

    # Calculate how much time it took to interact with all MPS.
    import time
    redis_time_data["global_total"]["total_redis_interaction_time"]["measured"][global_start_time] = ('%.3f' % (time.time() - float(global_start_time)))

    interactions = []
    for interaction,time in redis_time_data["global_total"]["total_redis_interaction_time"]["measured"].iteritems():
      interactions.append(float(time))

    redis_time_data['global_total']['total_redis_interaction_time']['mean'] = '%.3f' % np.mean(interactions)
    redis_time_data['global_total']['total_redis_interaction_time']['median'] = '%.3f' % np.median(interactions)
    redis_time_data['global_total']['total_redis_interaction_time']['std'] = '%.3f' % np.std(interactions)
    redis_time_data['global_total']['total_redis_interaction_time']['var'] = '%.3f' % np.var(interactions)
    redis_time_data['global_total']['total_redis_interaction_time']['min'] = '%.3f' % np.min(interactions)
    redis_time_data['global_total']['total_redis_interaction_time']['max'] = '%.3f' % np.max(interactions)
        
    for redis_server in redis_time_data['global_total']['per_redis_server_interaction_time'].iterkeys():
      interactions = []
      for interaction,time in redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]["measured"].iteritems():
        interactions.append(float(time))

      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]['mean'] = '%.3f' % np.mean(interactions)
      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]['median'] = '%.3f' % np.median(interactions)
      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]['std'] = '%.3f' % np.std(interactions)
      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]['var'] = '%.3f' % np.var(interactions)
      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]['min'] = '%.3f' % np.min(interactions)
      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]['max'] = '%.3f' % np.max(interactions)
    ############################ Calculate global statistics ############################
  try:
    with open(global_sysops_api_interaction_times, mode = "w") as fh:
      json.dump(redis_time_data['global_total'], fh, sort_keys=True, indent=3)
  except Exception, e:
    print "We tried to dump to JSON, but couldnt.  Sorry." + str(e)

#################################################################################################################
def interact_with_redis_server(key, file_data):
  old_handler = signal.signal(signal.SIGALRM, timeout_handler) 
  # set a 5 second alarm
  signal.alarm(5) 
  return_code = 0
  # Begin timeout exception
  try:
    return_code = push_redis_key_into_cache(key, file_data)
  except timeout_exception:
    print "=timeout_exception_redis_server=" + redis_server
    print "+redis_server_timeout_exception"
    return_code = 1
  finally:
    signal.signal(signal.SIGALRM, old_handler)
    signal.alarm(0)
  return return_code
#################################################################################################################
def push_redis_key_into_cache(key, file_data):
  tempkey = hostname + "$" + str(uuid.uuid4())
  try:
    redis_pipeline.expire(tempkey,ttl)
    for object in file_data:
      redis_pipeline.rpush(tempkey,bz2.compress(object))
    redis_pipeline.rename(tempkey, key)
    redis_pipeline.execute()
  except redis.exceptions.ConnectionError, e:
    print "=connection_error_redis_server=" + redis_server
    print "+redis_server_connection_failure"
    return 1
  return 0
#################################################################################################################
def produce_file_md5sum(data):
  d = hashlib.md5()
  d.update(data)
  return d.hexdigest()
#################################################################################################################
def produce_wc(data):
  num_of_chars = len(data) 
  num_of_lines = data.count('\n') + 1
  wordlist = data.split(None)
  num_of_words = len(wordlist)
  wc_tuple = "Number of characters: " + str(num_of_chars), "Number of lines: " + str(num_of_lines), "Number of words: " + str(num_of_words)
  return wc_tuple
#################################################################################################################
if __name__ == '__main__':
  """
  Populate the in-memory cache of all master policy servers that the client can communicate with.  This cache will be used by various tools from
  centralized servers to query the contents of the various configurations that are listed in /var/cfengine/outgoing.
  This cache replaces the rsync processes which doesnt scale.
  """
  parser = OptionParser(usage ="usage: %prog [options]",
    version ="%prog 1.0") 
  parser.add_option("-v", "--verbose",
    action = "store_true",
    dest = "verbose",
    default = False,
    help = "Enable verbose execution")
  parser.add_option("-p", "--primary-mps",
    action = "store",
    dest = "primary_mps",
    help = "The primary Cfengine master policy server for the client.")
  parser.add_option("-s", "--secondary-mps",
    action = "store",
    dest = "secondary_mps",
    help = "The secondary Cfengine master policy server for the client")
  parser.add_option("-t", "--third-mps",
    action = "store",
    dest = "third_mps",
    help = "The third Cfengine master policy server for the client.")
  parser.add_option("-f", "--forth-mps",
    action = "store",
    dest = "forth_mps",
    help = "The forth Cfengine master policy server for the client.")
  parser.add_option("-l", "--time-to-live",
    action = "store",
    dest = "ttl",
    help = "The time for objects to live in the cache.  By default, this is set to one hour.")

  (options,args) = parser.parse_args()

  if os.geteuid() != 0:
    print "You must be root to run this script.  This program executes several commands which require root privledges."
    sys.exit(1)

  if not options.primary_mps or not options.secondary_mps or not options.third_mps or not options.forth_mps:
    print "All for MPS options must be supplied in order to populate the cache.  Our architecture assumes data is replicated to all 4 MPS via this script."
    print "Even in small sites where two MPS are serving clients, we construct a python dictionary to perform the uniq for us so only two physcial machines"
    print "have data populated.  All four logical MPS must be filled.   Really, Cfengine automation should be the only user of this script."
    sys.exit(1)

  if options.ttl is None:
    ttl = 3600
  else:
    ttl = options.ttl

  if "linkedin.com" not in platform.node():
    hostname = platform.node() + ".linkedin.com"
  else:
    hostname = platform.node()

  redis_servers_response_codes = {}
  json_data_directory = "/etc/cfe.d/"

  list_of_files_command = "find /var/cfengine/outgoing -type l -ls | awk '{print $13}'"
  if options.verbose:
    print "(+) list of files command is " + list_of_files_command
  list_of_files,errors = subprocess.Popen(list_of_files_command,shell=True,stdout=subprocess.PIPE).communicate()

  # Even though we can provide up to 4x redis servers on the CLI, in most cases, they will be duplicated.  Our Cfengine architecture allows for up
  # to 4x MPS per core, but we could be in a small site with a single MPS.  In any case, build a dictionary object which does the "uniques" for us.
  # We use the key to find uniques.  We use the value to determine if that unique redis server is responsive or not.  If its unresponsive, we dont query it.

  redis_servers_response_codes[options.primary_mps] = 0
  redis_servers_response_codes[options.secondary_mps] = 0
  redis_servers_response_codes[options.third_mps] = 0
  redis_servers_response_codes[options.forth_mps] = 0
  
  redis_time_data = {}
  unique_files = {}
  unique_files['total' ] = {}
  global_start_time = '%.0f' % time.time()

  global_sysops_api_interaction_times = json_data_directory + "global_sysops_api_interaction_times.json"
  try:
    input_text = open(global_sysops_api_interaction_times).read()
    redis_time_data['global_total'] = json.loads(input_text.decode('utf-8', 'ignore'))
  except Exception, e:
    print "Sorry, couldnt read from the filesystem.  Creating a new object." + str(e)
    redis_time_data['global_total'] = {}
    redis_time_data['global_total']['total_redis_interaction_time'] = {}
    redis_time_data['global_total']['total_redis_interaction_time']['measured'] = {}
    redis_time_data['global_total']['per_redis_server_interaction_time'] = {}
  
  prune_array_data(redis_time_data['global_total']['total_redis_interaction_time'])

  for redis_server in redis_servers_response_codes.iterkeys():
    redis_server_start_time = time.time()

    # Load previous execution data from JSON.  If it doesn't exist, create new objects.
    individual_redis_server_interaction_times = json_data_directory + redis_server + "_sysops_api_interaction_times.json"
    try:
      input_text = open(individual_redis_server_interaction_times).read()
      redis_time_data[redis_server] = json.loads(input_text.decode('utf-8', 'ignore'))
    except Exception, e:
      print "Sorry, couldnt read from the filesystem.  Creating a new object." + str(e)
      redis_time_data[redis_server] = {}
      redis_time_data[redis_server]['measured'] = {}
      redis_time_data[redis_server]["individual_redis_server_totals"] = {}

    prune_array_data(redis_time_data[redis_server])
    redis_time_data[redis_server]["measured"][global_start_time] = {}

    if not redis_time_data['global_total']['per_redis_server_interaction_time'].get(redis_server):
      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server] = {}
      redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]["measured"] = {}

    prune_array_data(redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server])
    redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]["measured"][global_start_time] = {}

    redis_connection = redis.Redis(host=redis_server,port=6379,db=1,socket_timeout=5,charset='utf-8', errors='strict')
    redis_pipeline = redis_connection.pipeline()

    if options.verbose:
      print "(+) redis_server is " + redis_server
      print "(+) cache TTL is " + str(ttl)

    for file in list_of_files.splitlines():
      if os.path.isfile(file):
        if "wtmp" not in file and "last.log" not in file:
          file_data = []
          if "/etc/hardware_identification" in file:
            hardware_identification_data = open(file, 'r').read()
            hardware_identification  = json.loads(hardware_identification_data.decode('utf-8','ignore'))
            for hi_key in hardware_identification.iterkeys():
              key_start_time = time.time()
              file_data = []
              file = "/etc/hardware_identification.json" + "@" + hi_key
              key = hostname + "#" + file
              store_key = key.split("#")[1]
              file_data.append(str(hardware_identification[hi_key]))
              file_data.append(str(produce_file_md5sum(hardware_identification[hi_key])))
              file_data.append(str("No os.stat() is available.  hardware_identification is comprised of executed commands."))
	      file_data.append(str(produce_wc(hardware_identification[hi_key])))
	      # Push the obect into the cache.
              if not redis_servers_response_codes[redis_server]: 
                redis_servers_response_codes[redis_server] = interact_with_redis_server(key, file_data)
              # Total time to push the single key into the cache.
              redis_time_data[redis_server]["measured"][global_start_time][store_key] = '%.3f' % (time.time() - key_start_time)
            if options.verbose:
              print file + "\n\t" + redis_server + "\n\t" + key + "\n\t" + redis_time_data[redis_server]["measured"][global_start_time][store_key] + " seconds\n"
          else:
            key_start_time = time.time()
            key = hostname + "#" + file
            store_key = key.split("#")[1]
	    data = open(file,'rb').read()

            file_data.append(str(data))
            file_data.append(str(produce_file_md5sum(data)))
            file_data.append(str(os.stat(file)))
	    file_data.append(str(produce_wc(data)))
     
            # Make sure we aren't pushing objects larger than 10mb into RAM on the MPS.
            filesize = os.stat(file).st_size
            if filesize > 10485760:
              file_data[0] = "The file " + file + " exceeded 10mb in size. Not pushing this into Redis. Size was " + str(filesize / 1048576) + " megabytes"
	      key = hostname + "#" + file + "@" + "file_size_exceeded"
	    # Push the obect into the cache.
            if not redis_servers_response_codes[redis_server]: 
              redis_servers_response_codes[redis_server] = interact_with_redis_server(key, file_data)
            # Total time to push the single key into the cache.
            redis_time_data[redis_server]["measured"][global_start_time][store_key] = '%.3f' % (time.time() - key_start_time)
            if options.verbose:
              print file + "\n\t" + redis_server + "\n\t" + key + "\n\t" + redis_time_data[redis_server]["measured"][global_start_time][store_key] + " seconds\n"

    # Calculate how much time it took to interact with each MPS.
    redis_time_data[redis_server]["measured"][global_start_time]["redis_server_interaction_time"] = '%.3f' % (time.time() - redis_server_start_time)
    redis_time_data['global_total']['per_redis_server_interaction_time'][redis_server]["measured"][global_start_time] = redis_time_data[redis_server]["measured"][global_start_time]["redis_server_interaction_time"]
    if options.verbose:
      print "(+) Redis server " + redis_server + "  completed in " + redis_time_data[redis_server]["measured"][global_start_time]["redis_server_interaction_time"] + " seconds"

    start_calculation_time = time.time()
    generate_per_redis_server_execution_cost(redis_server)
    if options.verbose:
      print "(+) Calculated statistics from redis server " + redis_server + " in " + str('%.3f' % float(time.time() - start_calculation_time)) + " seconds"

    # Dump the per redis_server JSON data
    try:
      with open(individual_redis_server_interaction_times, mode = "w") as fh:
        json.dump(redis_time_data[redis_server], fh, sort_keys=True, indent=3)
    except Exception, e:
      print "We tried to dump to JSON, but couldnt.  Sorry." + str(e)

  # Calculate how much time it took to interact with all MPS.
  redis_time_data["global_total"]["total_redis_interaction_time"]["measured"][global_start_time] = ('%.3f' % (time.time() - float(global_start_time)))
 
  start_calculation_time = time.time()
  generate_global_redis_server_execution_cost()
  if options.verbose:
    print "(+) Calculated global statistics in " + str('%.3f' % float(time.time() - start_calculation_time)) + " seconds"
    print "(+) Execution completed in " + redis_time_data["global_total"]["total_redis_interaction_time"]["measured"][global_start_time] + " seconds"

  for redis_server in redis_servers_response_codes.iterkeys():
    if not redis_servers_response_codes[redis_server]:
      print "+successful_redis_server_connection"
      print "=successful_redis_server=" + redis_server
